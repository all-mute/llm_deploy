## Запуск LLM в облаке Yandex Cloud на GPU

Для оптимизации запуска LLM на виртуальных машинах и узлах с GPU можно использовать проект [vLLM](https://github.com/vllm-project/vllm). Этот проект поддерживает различные режимы запуска моделей, как с использованием квантизации, так и без, что позволяет сильно ускорить процесс работы моделей на достаточно мощном железе (например, NVidia A100).

В данной инструкции мы расскажем, как развернуть OpenAI Compatible Endpoint в облаке Yandex Cloud с помощью данного инструмента.

Развёртывание проще всего осуществить с помощью Docker-контейнера, в вычислительном узле DataSphere.

> Эта инструкция предполагает, что у вас есть доступ к Yandex Cloud, и вы умеете пользоваться основными облачными инструментами, такими, как `yc`, или веб-интерфейсом облака.

### Подготовка Docker-образа

> Действия ниже можно выполнять на локальном компьютере, или на временной виртуальной машине в облаке.

Для запуска vLLM в виде OpenAI-compatible endpoint существует готовый Docker-образ `vllm/vllm-openai`. Однако, для успешной работы необходимо передать ему в качестве аргумента имя модели в параметре `--model`. Поскольку DataSphere Node не поддерживает передачу параметра, нам придётся изготовить свой контейнер, который будет содержать в себе имя модели и прочие параметры сервера. 

1. Клонируйте данный репозиторий `git clone http://yandex-datasphere/llm_deploy`
2. Перейдите в папку `vLLM`: `cd vLLM`
3. Откройте `Dockerfile` и отредактируйте имя модели внутри команды `CMD` (параметр `--model`). Список поддерживаемых моделей есть [тут](https://docs.vllm.ai/en/latest/models/supported_models.html). Если вы планируете разворачивать образ на достаточно мощном узле семейства **g2.x** (A-100), то можно использовать флаг `-q awq`, вместе с [квантизированными моделями от TheBloke](https://huggingface.co/TheBloke?search_models=AWQ) (с суффиксом AWQ).
4. Соберите Docker-образ с помощью следующей команды:
    ```bash
    docker build -t vllm .
    ```

Запустить этот образ локально Вы сможете только на виртуальной машине с GPU, поскольку vLLM не поддерживает запуск моделей на CPU.

### Настройка облачного каталога и сервисного аккаунта

Для разворачивания ноды и хранения Docker-образа вам потребуется некоторый каталог в облаке Yandex Cloud.

> Если у вас не установлен интерфейс командной строки Yandex Cloud, то выполните шаги [в этой инструкции](https://cloud.yandex.ru/ru/docs/cli/quickstart#install).

1. Вы можете создать новый каталог для данного развёртывания, или использовать каталог по умолчанию (Default). Запомните название каталога, оно вам понадобится ниже.
1. Задайте этот каталог каталогом по умолчанию: `yc config set folder-name <название_каталога>`.
1. В каталоге создайте сервисный аккаунт с необходимыми правами доступа - на чтение/запись в Container Registry, на работу с DataSphere.
1. Для сервисного аккаунта создайте json-файл с ключём доступа, он вам потом понадобится:
    ```bash
    yc iam key create --service-account-name <имя_сервисного_аккаунта> \ 
        --output key.json --folder-id <ID_каталога>
    ```
1. Файл должен иметь примерно следующий вид:
    ```json
    {
      "id": "...",
      "service_account_id": "...",
      "created_at": "2024-03-05T15:13:23.110805042Z",
      "key_algorithm": "RSA_2048",
      "public_key": "...",
      "private_key": "..."
    }
    ```

### Помещаем Docker-образ в Container Registry

1. Для хранения Docker-образа создайте в облаке реестр Container Registry, если у вас её пока нет. Скопируйте идентификатор реестра.
    > Идентификатор реестра можно посмотреть, выполнив команду `yc container registry list`
1. Получите IAM-токен для своего пользовательского аккаунта: `yc iam create-token`.
1. Войдите в контейнерный реестр, выполнив команду ниже (подставьте вместо <IAM-токен> значение токена с предыдущего шага):
    ```bash
    docker login \
    --username iam \
    --password <IAM-токен> \
    cr.yandex
    ```
1. Загрузите Docker-образ в Container Registry:
    ```bash
    docker tag vllm cr.yandex/<идентификатор_реестра>/vllm:latest
    docker push cr.yandex/<идентификатор_реестра>/vllm:latest
    ```
> При необходимости можно использовать Docker Hub или другой реестр для хранения образа.

### Создаём ноду DataSphere

1. В настройках проекта DataSphere обязательно укажите каталог по умолчанию и сервисный аккаунт, который вы используете. 
1. В проекте DataSphere создайте новый ресурс "Нода" из Docker-образа. При создании укажите следующие параметры:
    * Имя: <имя_ноды>
    * Тип: Docker-образ
    * Путь к образу: `cr.yandex/<id_container_registry>/vllm:latest`
    * Порт: 8000
    * Таймаут: 180 секунд
    * Для того, чтобы нода могла получить доступ к container registry, в дополнительных параметрах укажите:
        - Имя пользователя: `json_key`
        - Секрет с паролем: создайте секрет в DataSphere, содержащий **всё содержимое** файла `json.key`, созданного выше.
1. Для тестирования развернутой ноды, можно перейти во вкладку **запрос** и сделать GET-запрос по адресу `/v1/models` - в результате вы получите в ответ список моделей в формате JSON.
    > Обратите внимание, что для запуска ноды может потребоваться достаточно большое время на скачивание Docker-образа из реестра, а также на запуск самой модели. Поэтому первое время вы можете наблюдать ошибку 503 при выполнении запроса, хотя статус ноды будет **Healthy**. Нужно немного подождать. 
1. Вы также можете сделать запрос из интернет, не забывая указывать параметры авторизации облака (IAM-токен), а также идентификатор ноды и облачного каталога:
    ```
    curl -H "x-node-id: <id_ноды>" -H "Authorization: Bearer <IAM_TOKEN>" -H "x-folder-id: <id_каталога>" https://node-api.datasphere.yandexcloud.net/invoke/v1/models
    ```

### Устранение ошибок

В случае возникновения ошибок, для их устранения можно смотреть в логи ноды, чтобы понять, чем вызвана ошибка. Наиболее частые варианты:
1. Вы не указали облачный каталог и сервисный аккаунт в проекте DataSphere
1. Вы не указали параметр `json_key` и соответствующий секрет при создании ноды, из-за чего контейнер не может скачаться из реестра.
1. Вы указали слишком крутую модель, которая не помещается в память GPU. Вы увидите соответствующее сообщение об ошибке, когда посмотрите логи запуска ноды.

### Результат

Теперь к развёрнутому сервису можно обращаться через endpoint, совместимый с OpenAI. Единственной существенной особенностью является то, что необходимо для авторизации передавать дополнительные данные (ID ноды, ID каталога и IAM-токен) в заголовке запроса.

Пример того, как обращаться к развёрнутой в DataSphere модели через LangChain, содержится в [`../examples/langchain_demo.py`](../examples/langchain_demo.py). Прежде, чем запускать пример, задайте правильные значения переменных `iam_token`, `folder_id` и `node_id` в начале файла.

Для балансировки нагрузки между нодами и обновления развернутых сервисов во время работы создайте алиас.
